{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ae7e0f6",
   "metadata": {},
   "source": [
    "# Final Project Tasks.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e362517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install langchain_community pypdfium2 ibm-watsonx-ai langchain-ibm chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18202ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To filter out warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812fdd72",
   "metadata": {},
   "source": [
    "## Task 1 - PDF Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b76bbbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Comprehensive Review of Low-Rank\n",
      "Adaptation in Large Language Models for\n",
      "Efficient Parameter Tuning\n",
      "September 10, 2024\n",
      "Abstract\n",
      "Natural Language Processing (NLP) often involves pre-training large\n",
      "models on extensive datasets and then adapting them for specific tasks\n",
      "through fine-tuning. However, as these models grow larger, like GPT-3\n",
      "with 175 billion parameters, fully fine-tuning them becomes computa\u0002tionally expensive. We propose a novel method called LoRA (Low-Rank\n",
      "Adaptation) that significantly reduces the overhead by freezing the orig\u0002inal model weights and only training small rank decomposition matrices.\n",
      "This leads to up to 10,000 times fewer trainable parameters and reduces\n",
      "GPU memory usage by three times. LoRA not only maintains but some\u0002times surpasses fine-tuning performance on models like RoBERTa, De\u0002BERTa, GPT-2, and GPT-3. Unlike other methods, LoRA introduces\n",
      "no extra latency during inference, making it more efficient for practical\n",
      "applications. All relevant code and mo\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from langchain_community.document_loaders import PyPDFium2Loader\n",
    "\n",
    "# Define the URL of the PDF document\n",
    "paper_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/WgM1DaUn2SYPcCg_It57tA/A-Comprehensive-Review-of-Low-Rank-Adaptation-in-Large-Language-Models-for-Efficient-Parameter-Tuning-1.pdf\"\n",
    "\n",
    "# Load the PDF document using PyPDFium2Loader\n",
    "pdf_loader = PyPDFium2Loader(paper_url)\n",
    "\n",
    "# Load and split the PDF document into pages\n",
    "pages = pdf_loader.load_and_split()\n",
    "print(pages[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd589f66",
   "metadata": {},
   "source": [
    "## Task 2 - Code Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02048dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_text = \"\"\"\n",
    "    \\documentclass{article}\n",
    "    \n",
    "    \\begin{document}\n",
    "    \n",
    "    \\maketitle\n",
    "    \n",
    "    \\section{Introduction}\n",
    "    Large language models (LLMs) are a type of machine learning model that can be trained on vast amounts of text data to generate human-like language. In recent years, LLMs have made significant advances in various natural language processing tasks, including language translation, text generation, and sentiment analysis.\n",
    "    \n",
    "    \\subsection{History of LLMs}\n",
    "    The earliest LLMs were developed in the 1980s and 1990s, but they were limited by the amount of data that could be processed and the computational power available at the time. In the past decade, however, advances in hardware and software have made it possible to train LLMs on massive datasets, leading to significant improvements in performance.\n",
    "    \n",
    "    \\subsection{Applications of LLMs}\n",
    "    LLMs have many applications in the industry, including chatbots, content creation, and virtual assistants. They can also be used in academia for research in linguistics, psychology, and computational linguistics.\n",
    "    \n",
    "    \\end{document}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9181cca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='\\\\documentclass{article}\\n    \\n    \\x08egin{document}'),\n",
       " Document(metadata={}, page_content='\\\\maketitle\\n    \\n    \\\\section{Introduction}\\n    Large'),\n",
       " Document(metadata={}, page_content='language models (LLMs) are a type of machine learning model'),\n",
       " Document(metadata={}, page_content='that can be trained on vast amounts of text data to'),\n",
       " Document(metadata={}, page_content='generate human-like language. In recent years, LLMs have'),\n",
       " Document(metadata={}, page_content='made significant advances in various natural language'),\n",
       " Document(metadata={}, page_content='processing tasks, including language translation, text'),\n",
       " Document(metadata={}, page_content='generation, and sentiment analysis.'),\n",
       " Document(metadata={}, page_content='\\\\subsection{History of LLMs}\\n    The earliest LLMs were'),\n",
       " Document(metadata={}, page_content='developed in the 1980s and 1990s, but they were limited by'),\n",
       " Document(metadata={}, page_content='the amount of data that could be processed and the'),\n",
       " Document(metadata={}, page_content='computational power available at the time. In the past'),\n",
       " Document(metadata={}, page_content='decade, however, advances in hardware and software have'),\n",
       " Document(metadata={}, page_content='made it possible to train LLMs on massive datasets, leading'),\n",
       " Document(metadata={}, page_content='to significant improvements in performance.'),\n",
       " Document(metadata={}, page_content='\\\\subsection{Applications of LLMs}\\n    LLMs have many'),\n",
       " Document(metadata={}, page_content='applications in the industry, including chatbots, content'),\n",
       " Document(metadata={}, page_content='creation, and virtual assistants. They can also be used in'),\n",
       " Document(metadata={}, page_content='academia for research in linguistics, psychology, and'),\n",
       " Document(metadata={}, page_content='computational linguistics.\\n    \\n    \\\\end{document}')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the necessary module for text splitting\n",
    "from langchain.text_splitter import Language, RecursiveCharacterTextSplitter\n",
    "\n",
    "# Create a RecursiveCharacterTextSplitter instance for LaTeX documents\n",
    "latex_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.LATEX, chunk_size=60, chunk_overlap=0\n",
    ")\n",
    "\n",
    "# Split the LaTeX text into documents\n",
    "latex_docs = latex_splitter.create_documents([latex_text])\n",
    "latex_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abb041d",
   "metadata": {},
   "source": [
    "## Task 3 - Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb99fd30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.018171897, -0.018608226, 0.059054308, 0.07260351, 0.08736516]\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary module for embedding\n",
    "from ibm_watsonx_ai.metanames import EmbedTextParamsMetaNames\n",
    "from langchain_ibm import WatsonxEmbeddings\n",
    "\n",
    "# Define the embedding parameters\n",
    "embedding_params = {\n",
    "    EmbedTextParamsMetaNames.TRUNCATE_INPUT_TOKENS: 3,\n",
    "    EmbedTextParamsMetaNames.RETURN_OPTIONS: {\"input_text\": True},\n",
    "}\n",
    "\n",
    "# Initialize the WatsonxEmbeddings with the specified model and parameters\n",
    "watsonx_embedding = WatsonxEmbeddings(\n",
    "    model_id=\"ibm/slate-30m-english-rtrvr\",\n",
    "    url=\"https://us-south.ml.cloud.ibm.com\",\n",
    "    project_id=\"skills-network\",\n",
    "    params=embedding_params,\n",
    ")\n",
    "\n",
    "# Embed the given query using the WatsonxEmbeddings instance\n",
    "query = \"How are you?\"\n",
    "query_result = watsonx_embedding.embed_query(query)\n",
    "print(query_result[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58efd47",
   "metadata": {},
   "source": [
    "## Task 4 - Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c734c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1: This policy encourages the responsible use of mobile devices in line with legal and ethical\n",
      "Result 2: This policy promotes the safe and responsible use of digital communication tools in line with our\n",
      "Result 3: This policy lays the foundation for a diverse, inclusive, and talented workforce. It ensures that\n",
      "Result 4: guidelines. The policy is regularly reviewed to stay current with evolving technology and security\n",
      "Result 5: We encourage a culture of safety, including reporting any unsafe practices or conditions.\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary module for vector stores\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Load the text document using TextLoader\n",
    "text_loader = TextLoader(\"data/new-Policies.txt\")\n",
    "data = text_loader.load()\n",
    "\n",
    "# Split the loaded text document into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    ")\n",
    "chunks = text_splitter.split_documents(data)\n",
    "ids = [str(i) for i in range(0, len(chunks))]\n",
    "\n",
    "# Define the embedding parameters for the WatsonxEmbeddings\n",
    "embedding_params = {\n",
    "    EmbedTextParamsMetaNames.TRUNCATE_INPUT_TOKENS: 3,\n",
    "    EmbedTextParamsMetaNames.RETURN_OPTIONS: {\"input_text\": True},\n",
    "}\n",
    "\n",
    "# Initialize the WatsonxEmbeddings with the specified model and parameters\n",
    "watsonx_embedding = WatsonxEmbeddings(\n",
    "    model_id=\"ibm/slate-125m-english-rtrvr\",\n",
    "    url=\"https://us-south.ml.cloud.ibm.com\",\n",
    "    project_id=\"skills-network\",\n",
    "    params=embedding_params,\n",
    ")\n",
    "\n",
    "# Create a Chroma vector store from the chunks and their embeddings\n",
    "vectordb = Chroma.from_documents(chunks, watsonx_embedding, ids=ids)\n",
    "\n",
    "# Perform a similarity search in the vector store using a query\n",
    "query = \"Smoking policy\"\n",
    "docs = vectordb.similarity_search(query, k=5)\n",
    "for i, doc in enumerate(docs, start=1):\n",
    "    print(f\"Result {i}: {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2257b6",
   "metadata": {},
   "source": [
    "## Task 5 - Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "006ff951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1: and email use, including copyright and data protection laws.\n",
      "Result 2: This policy encourages the responsible use of mobile devices in line with legal and ethical\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary module for vector stores\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Load the text document using TextLoader\n",
    "text_loader = TextLoader(\"data/new-Policies.txt\")\n",
    "data = text_loader.load()\n",
    "\n",
    "# Split the loaded text document into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    ")\n",
    "chunks = text_splitter.split_documents(data)\n",
    "ids = [str(i) for i in range(0, len(chunks))]\n",
    "\n",
    "# Define the embedding parameters for the WatsonxEmbeddings\n",
    "embedding_params = {\n",
    "    EmbedTextParamsMetaNames.TRUNCATE_INPUT_TOKENS: 3,\n",
    "    EmbedTextParamsMetaNames.RETURN_OPTIONS: {\"input_text\": True},\n",
    "}\n",
    "\n",
    "# Initialize the WatsonxEmbeddings with the specified model and parameters\n",
    "watsonx_embedding = WatsonxEmbeddings(\n",
    "    model_id=\"ibm/slate-125m-english-rtrvr\",\n",
    "    url=\"https://us-south.ml.cloud.ibm.com\",\n",
    "    project_id=\"skills-network\",\n",
    "    params=embedding_params,\n",
    ")\n",
    "\n",
    "# Create a Chroma vector store from the chunks and their embeddings\n",
    "vectordb = Chroma.from_documents(chunks, watsonx_embedding, ids=ids)\n",
    "\n",
    "# Create a retriever from the vector store\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "# Use the retriever to find relevant documents for a given query\n",
    "query = \"Email policy\"\n",
    "docs = retriever.invoke(query)\n",
    "for i, doc in enumerate(docs, start=1):\n",
    "    print(f\"Result {i}: {doc.page_content}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
